{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Integration ðŸ¦œðŸ”— "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMQL can also be used together with the [ðŸ¦œðŸ”— LangChain](https://python.langchain.com/en/latest/index.html#) python library. Both, using langchain libraries from LMQL code and using LMQL queries as part of chains are supported."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain from LMQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider the case, where one may want to use LangChain modules as part of an LMQL query.\n",
    "\n",
    "In this example, we want to leverage the LangChain `Chroma` retrieval model, to enable question answering about a text document (the LMQL paper in this case).\n",
    "\n",
    "First, we need to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup lmql path (not shown in documentation, metadata has nbshpinx: hidden)\n",
    "import sys \n",
    "sys.path.append(\"../../../src/\")\n",
    "# load and set OPENAI_API_KEY\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = open(\"../../../api.env\").read().split(\"\\n\")[1].split(\": \")[1].strip()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "import asyncio\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load and embed the text of the relevant document (`lmql.txt` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text of LMQL paper\n",
    "with open(\"lmql.txt\") as f:\n",
    "    contents = f.read()\n",
    "texts = []\n",
    "for i in range(0, len(contents), 120):\n",
    "    texts.append(contents[i:i+120])\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_texts(texts, embeddings, \n",
    "    metadatas=[{\"text\": t} for t in texts], persist_directory=\"lmql-index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a chatbot function, using a simple LMQL query, that first prompts the user for a question via `await input(...)`, retrieves relevant text paragraphs using LangChain and then produces an answer using `openai/gpt-3.5-turbo` (ChatGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is LMQL?\n",
      "\u001b[32mReading relevant pages...\u001b[0m\n",
      "LMQL stands for Model Query Language. It is a high-level language that combines declarative SQL-like elements with an imperative syntax.\n",
      "Question: How does it work?\n",
      "\u001b[32mReading relevant pages...\u001b[0m\n",
      "LMQL works by executing all calls in lockstep, which allows for batching calls to the underlying model. This ensures that the queries are executed non-deterministically."
     ]
    }
   ],
   "source": [
    "import termcolor\n",
    "\n",
    "@lmql.query\n",
    "async def chatbot():\n",
    "        '''lmql\n",
    "sample(temperature=0.2, max_len=2048, openai_chunksize=2048)\n",
    "    \"\"\"You are a chatbot that helps users answer questions.\n",
    "    You are first provided with the question and relevant information.\"\"\"\n",
    "    while True:\n",
    "        q = await input(\"\\nQuestion: \")\n",
    "        if q == \"exit\": break\n",
    "        \"Question: {q}\\n\"\n",
    "        print(termcolor.colored(\"Reading relevant pages...\", \"green\"))\n",
    "        results = set([d.page_content for d in docsearch.similarity_search(q, 4)])\n",
    "        information = \"\\n\\n\".join([\"...\" + r + \"...\" for r in list(results)])\n",
    "        \"\\nRelevant Information: {information}\\n\"\n",
    "        \"Your response based on relevant information:[RESPONSE]\"\n",
    "from\n",
    "    \"openai/gpt-3.5-turbo\"\n",
    "        '''\n",
    "\n",
    "await chatbot(output_writer=lmql.stream(variable=\"RESPONSE\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the query, inline LMQL code appearing in a Python script can access the outer scope containing e.g. the `docsearch` variable, and access any relevant utility functions and object defined in Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LMQL from LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using langchain utilities in LMQL query code, LMQL queries can also seamlessly be integrated as a `langchain` `Chain` component, or if necessary used as an LMTP client in place of a `langchain` `LLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LMQL as a Chain component"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where possible, the preferred usage is as a `langchain` `Chain` component. For this, consider the sequential prompting example from the `langchain` documentation, where we first prompt the language model to propose a company name for a given product, and then ask it for a catchphrase.\n",
    "\n",
    "To get started, we first import the relevant langchain components, as well as LMQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (ChatPromptTemplate,HumanMessagePromptTemplate)\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import lmql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chain has two stages: (1) Asking the model for a company name, and (2) asking the model for a catchphrase. For the sake of this example, we will implement (1) in with a langchain prompt and (2) with an LMQL query. \n",
    "\n",
    "First, we define the langchain prompt for the company name and instantiate the resulting `LLMChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the LM to be used by langchain\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"What is a good name for a company that makes {product}?\",\n",
    "            input_variables=[\"product\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can already be executed to produce a company name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rainbow Socks Co.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define prompt (2) in LMQL, i.e. the LMQL query generating the catchphrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lmql.query\n",
    "async def write_catch_phrase(company_name: str):\n",
    "    '''\n",
    "    argmax \"Write a catchphrase for the following company: {company_name}. [catchphrase]\" from \"chatgpt\"\n",
    "    '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can run this part in isolation, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Step up your style with Socks Inc.\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(await write_catch_phrase(\"Socks Inc\"))[0].variables[\"catchphrase\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To chain the two prompts together, we can use a `SimpleSequentialChain` from `langchain`. To make an LMQL query compatible for use with `langchain`, just call `.aschain()` on it, before passing it to the `SimpleSequentialChain` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, write_catch_phrase.aschain()], verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the overall chain, relying both on LMQL and langchain components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"colorful socks\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> Entering new SimpleSequentialChain chain...\n",
    "RainbowSocks Co.\n",
    " \"Step into a world of color with RainbowSocks Co.!\"\n",
    "\n",
    "> Finished chain.\n",
    " \"Step into a world of color with RainbowSocks Co.!\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the full chain currently only works when run outside a Jupyter environment (in a script), due to the way langchain and LMQL rely on async vs. sequential calls. With further improvements to the `langchain` async API, this limitation can be expected to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LMQL as an LMTP client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LMQL as an intermediate `langchain` `LLM` has been implemented as an temporary stage in bringing LMTP proper to `langchain`. Currently, this usage supports:\n",
    "+ sync and async `LLM` calls\n",
    "+ deterministic or temperature-based sampling\n",
    "+ completion token count limits\n",
    "+ stop word lists (accelerated by `LMQL`'s `STOPS_BEFORE()` constraint.)\n",
    "+ Langchain callbacks (modeled off the langchain support for `ctransformers`, so limited to the extent of `ctransformers` support.)\n",
    "\n",
    "To get started, we import the LMQL langchain support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmql.runtime.langchain.llm import LMQL as LMQL_LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate an LMQL `LLM` instance, which is a drop in for `langchain` `LLM` instances for most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmql_llm = LMQL_LLM(\n",
    "    model=\"openai/chatgpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the instance with any `langchain` systems desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await lmql_llm.agenerate(\n",
    "    \"## Intro: The\",\n",
    "    stop=[\"\\n\", \".\"],\n",
    "    temperature=0.9,\n",
    "    max_length = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " problem of identifying a subset of features in a data set that contribute significantly to the overall predictive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `temperature` and `max_length` keywords may also be set during the `LLM` instantiation. Call-site keywords take precedence. If `temperature` is not given, the query will use `argmax` sampling. If `max_length` is not given, the generated result will not be truncated. (Beware running over the context length set on the LLM you're using as your LMQL model, as this frequently causes unrecoverable errors that may lock up the program.) `max_length` is counted as number of _newly generated_ tokens, not as total number of tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
